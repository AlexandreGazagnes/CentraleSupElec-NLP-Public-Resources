{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 012-03 - NLP Text Processing - Solution Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Written by Alexandre Gazagnes\n",
    "* Last update: 2024-02-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna implement our 1st NLP tool ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data  : \n",
    "\n",
    "**You can find the dataset [here](https://gist.githubusercontent.com/AlexandreGazagnes/cabe445634a092d308d17a883a305a75/raw/9f785f0f02739ac6352e1d583323771d55270221/nlp.csv).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "PBabb1xmqSlM"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dAhj129JqdaC"
   },
   "source": [
    "### System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands will display the system information:\n",
    "\n",
    "Uncomment theses lines if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yE-usmhkycvz"
   },
   "source": [
    "Install various Librairies : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:57.038920Z",
     "start_time": "2023-02-23T06:58:57.032717Z"
    },
    "hidden": true,
    "id": "sfsFooZVqajD"
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt >> pip.log\n",
    "# !pip freeze >> pip.freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "xVpywqWAqalg"
   },
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:59.595289Z",
     "start_time": "2023-02-23T06:58:57.672688Z"
    },
    "hidden": true,
    "id": "Rt4va8LsqnGR"
   },
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "import pickle\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.impute import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.feature_extraction import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# from lightgbm import *\n",
    "# from xgboost import *\n",
    "\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# import wordcloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "xkZ4K9CRqaoM"
   },
   "source": [
    "### Graphs and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:59.600629Z",
     "start_time": "2023-02-23T06:58:59.597369Z"
    },
    "hidden": true,
    "id": "34QZa76rqxLu"
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:59.604999Z",
     "start_time": "2023-02-23T06:58:59.602475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action=\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:59.610007Z",
     "start_time": "2023-02-23T06:58:59.607152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DISPLAY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Thrid Parties Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some Third parties : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:00.115507Z",
     "start_time": "2023-02-23T06:59:00.044665Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some string assets : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:00.491350Z",
     "start_time": "2023-02-23T06:59:00.420416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "# stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "punctuation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = words.words()\n",
    "word_dict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download spacy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:01.023888Z",
     "start_time": "2023-02-23T06:59:01.016385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm >> spacy.log\n",
    "\n",
    "!python -m spacy download en_core_web_md >> spacy.log\n",
    "# !python -m spacy download en_core_web_lg >> spacy.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load spacy model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:01.130801Z",
     "start_time": "2023-02-23T06:59:01.128456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "FUVeEfm9qx8f"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url of the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:01.819218Z",
     "start_time": "2023-02-23T06:59:01.811890Z"
    },
    "hidden": true,
    "id": "o_SlDzD1r4KP"
   },
   "outputs": [],
   "source": [
    "url = \"https://gist.githubusercontent.com/AlexandreGazagnes/cabe445634a092d308d17a883a305a75/raw/d2014e8a34bba3c1be3ec8936bb338fb42888f24/nlp.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:02.206962Z",
     "start_time": "2023-02-23T06:59:02.172398Z"
    },
    "hidden": true,
    "id": "4LfAEaWRrzTz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>home furnishing</td>\n",
       "      <td>curtains &amp; accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sathiyas Cotton Bath Towel</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>baby care</td>\n",
       "      <td>baby bath &amp; skin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King size...</td>\n",
       "      <td>Key Features of SANTOSH ROYAL FASHION Cotton P...</td>\n",
       "      <td>home furnishing</td>\n",
       "      <td>bed linen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double B...</td>\n",
       "      <td>Key Features of Jaipur Print Cotton Floral Kin...</td>\n",
       "      <td>home furnishing</td>\n",
       "      <td>bed linen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Camerii WM64 Elegance Analog Watch  - For Men,...</td>\n",
       "      <td>Camerii WM64 Elegance Analog Watch  - For Men,...</td>\n",
       "      <td>watches</td>\n",
       "      <td>wrist watches</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Elegance Polyester Multicolor Abstract Eyelet ...   \n",
       "1                         Sathiyas Cotton Bath Towel   \n",
       "2  SANTOSH ROYAL FASHION Cotton Printed King size...   \n",
       "3  Jaipur Print Cotton Floral King sized Double B...   \n",
       "4  Camerii WM64 Elegance Analog Watch  - For Men,...   \n",
       "\n",
       "                                         description            cat_1  \\\n",
       "0  Key Features of Elegance Polyester Multicolor ...  home furnishing   \n",
       "1  Specifications of Sathiyas Cotton Bath Towel (...        baby care   \n",
       "2  Key Features of SANTOSH ROYAL FASHION Cotton P...  home furnishing   \n",
       "3  Key Features of Jaipur Print Cotton Floral Kin...  home furnishing   \n",
       "4  Camerii WM64 Elegance Analog Watch  - For Men,...          watches   \n",
       "\n",
       "                    cat_2  \n",
       "0  curtains & accessories  \n",
       "1        baby bath & skin  \n",
       "2               bed linen  \n",
       "3               bed linen  \n",
       "4           wrist watches  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_1\n",
       "kitchen & dining              150\n",
       "baby care                     149\n",
       "home furnishing               148\n",
       "watches                       147\n",
       "beauty and personal care      147\n",
       "home decor & festive needs    145\n",
       "computers                     145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cat_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1031, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a copy of the df : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T14:50:27.970009Z",
     "start_time": "2023-02-10T14:50:27.432446Z"
    },
    "heading_collapsed": true,
    "id": "kzoLQvf44HRX"
   },
   "source": [
    "## First tour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 10 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:04.207912Z",
     "start_time": "2023-02-23T06:59:04.190392Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Nutcase Sticker Wrap Design - Pineapple 800 ml...</td>\n",
       "      <td>Nutcase Sticker Wrap Design - Pineapple 800 ml...</td>\n",
       "      <td>kitchen &amp; dining</td>\n",
       "      <td>containers &amp; bottles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Sonata super fiber Digital Watch  - For Boys, ...</td>\n",
       "      <td>Sonata super fiber Digital Watch  - For Boys, ...</td>\n",
       "      <td>watches</td>\n",
       "      <td>wrist watches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Craftuno Handcrafted Paper Mache Box - Set of ...</td>\n",
       "      <td>Buy Craftuno Handcrafted Paper Mache Box - Set...</td>\n",
       "      <td>beauty and personal care</td>\n",
       "      <td>makeup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>Utsav Handicraft UHD004 Showpiece  -  8 cm</td>\n",
       "      <td>Utsav Handicraft UHD004 Showpiece  -  8 cm (Si...</td>\n",
       "      <td>home decor &amp; festive needs</td>\n",
       "      <td>table decor &amp; handicrafts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Rockmantra Red Flowers In Bucket Ceramic Mug</td>\n",
       "      <td>Rockmantra Red Flowers In Bucket Ceramic Mug (...</td>\n",
       "      <td>kitchen &amp; dining</td>\n",
       "      <td>coffee mugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Vitamins Graphic Print Baby Girl's Basic Shorts</td>\n",
       "      <td>Key Features of Vitamins Graphic Print Baby Gi...</td>\n",
       "      <td>baby care</td>\n",
       "      <td>infant wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>Noise NOSWW001 Analog Watch  - For Men, Women</td>\n",
       "      <td>Noise NOSWW001 Analog Watch  - For Men, Women ...</td>\n",
       "      <td>watches</td>\n",
       "      <td>wrist watches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Maxima 20981LMGI Attivo Analog Watch  - For Men</td>\n",
       "      <td>Maxima 20981LMGI Attivo Analog Watch  - For Me...</td>\n",
       "      <td>watches</td>\n",
       "      <td>wrist watches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Riva Carpets Cotton Free Bath Mat Classic Loop...</td>\n",
       "      <td>Buy Riva Carpets Cotton Free Bath Mat Classic ...</td>\n",
       "      <td>home furnishing</td>\n",
       "      <td>bath linen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Perucci PC-303 Analog Watch  - For Men</td>\n",
       "      <td>Perucci PC-303 Analog Watch  - For Men - Buy P...</td>\n",
       "      <td>watches</td>\n",
       "      <td>wrist watches</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name  \\\n",
       "727  Nutcase Sticker Wrap Design - Pineapple 800 ml...   \n",
       "563  Sonata super fiber Digital Watch  - For Boys, ...   \n",
       "969  Craftuno Handcrafted Paper Mache Box - Set of ...   \n",
       "914         Utsav Handicraft UHD004 Showpiece  -  8 cm   \n",
       "133       Rockmantra Red Flowers In Bucket Ceramic Mug   \n",
       "70     Vitamins Graphic Print Baby Girl's Basic Shorts   \n",
       "511      Noise NOSWW001 Analog Watch  - For Men, Women   \n",
       "508    Maxima 20981LMGI Attivo Analog Watch  - For Men   \n",
       "31   Riva Carpets Cotton Free Bath Mat Classic Loop...   \n",
       "746             Perucci PC-303 Analog Watch  - For Men   \n",
       "\n",
       "                                           description  \\\n",
       "727  Nutcase Sticker Wrap Design - Pineapple 800 ml...   \n",
       "563  Sonata super fiber Digital Watch  - For Boys, ...   \n",
       "969  Buy Craftuno Handcrafted Paper Mache Box - Set...   \n",
       "914  Utsav Handicraft UHD004 Showpiece  -  8 cm (Si...   \n",
       "133  Rockmantra Red Flowers In Bucket Ceramic Mug (...   \n",
       "70   Key Features of Vitamins Graphic Print Baby Gi...   \n",
       "511  Noise NOSWW001 Analog Watch  - For Men, Women ...   \n",
       "508  Maxima 20981LMGI Attivo Analog Watch  - For Me...   \n",
       "31   Buy Riva Carpets Cotton Free Bath Mat Classic ...   \n",
       "746  Perucci PC-303 Analog Watch  - For Men - Buy P...   \n",
       "\n",
       "                          cat_1                      cat_2  \n",
       "727            kitchen & dining       containers & bottles  \n",
       "563                     watches              wrist watches  \n",
       "969    beauty and personal care                     makeup  \n",
       "914  home decor & festive needs  table decor & handicrafts  \n",
       "133            kitchen & dining                coffee mugs  \n",
       "70                    baby care                infant wear  \n",
       "511                     watches              wrist watches  \n",
       "508                     watches              wrist watches  \n",
       "31              home furnishing                 bath linen  \n",
       "746                     watches              wrist watches  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1031 entries, 0 to 1030\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         1031 non-null   object\n",
      " 1   description  1031 non-null   object\n",
      " 2   cat_1        1031 non-null   object\n",
      " 3   cat_2        1031 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 32.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value counts : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific data types : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude=np.number).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Nan & Dupliacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any missing values : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = df.isna().mean(axis=0)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = df.isna().mean(axis=1)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any duplicated : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numerical stats : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other stats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe(include=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select numeric : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(np.number).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select non numeric : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(object).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Text Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[print(i + \"\\n\\n\") for i in df.description.head().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[print(i + \"\\n\\n\") for i in df.description.tail().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[print(i + \"\\n\\n\") for i in df.description.sample(10).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display by cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "\n",
    "key = df.cat_1.unique()[i]\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------------------\")\n",
    "print(f\"-------------- {key} --------------- \")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "tmp = df.loc[df.cat_1 == key, :]\n",
    "[print(i[:lim] + \"\\n\\n\") for i in tmp.description.head(5).values]\n",
    "[print(i[:lim] + \"\\n\\n\") for i in tmp.description.sample(5).values]\n",
    "[print(i[:lim] + \"\\n\\n\") for i in tmp.description.tail(5).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_categ(i):\n",
    "\n",
    "    key = df.cat_1.unique()[i]\n",
    "\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(f\"-------------- {key} --------------- \")\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    tmp = df.loc[df.cat_1 == key, :]\n",
    "    [print(i[:lim] + \"\\n\\n\") for i in tmp.description.head(5).values]\n",
    "    [print(i[:lim] + \"\\n\\n\") for i in tmp.description.sample(5).values]\n",
    "    [print(i[:lim] + \"\\n\\n\") for i in tmp.description.tail(5).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Categ 1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_categ(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Categ 2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_categ(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Text To Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create doc from 1st description : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df.description.iloc[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Many Tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Stop words : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our punctuation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English dictionnary (lower) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict\n",
    "word_dict = [i.lower() for i in word_dict]\n",
    "word_dict[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenizer(\n",
    "    doc: str,\n",
    "    len_min_word: int = 3,\n",
    "    force_lower: bool = True,\n",
    "    remove_stop_words=True,\n",
    "    remove_punct=True,\n",
    "    remove_all_digit=True,\n",
    "    remove_any_digit=False,\n",
    "    list_dict_word=None,\n",
    "    list_extra_stop_word=None,\n",
    "    remove_duplicate=False,\n",
    ") -> str:\n",
    "\n",
    "    if force_lower:\n",
    "        doc = doc.lower()  # if force_lower else doc\n",
    "\n",
    "    doc = doc.strip()\n",
    "\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "\n",
    "    if remove_stop_words:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    if remove_punct:\n",
    "        tokens = [t for t in tokens if t not in punctuation]\n",
    "\n",
    "    if len_min_word > 0:\n",
    "        tokens = [t for t in tokens if len(t) >= len_min_word]\n",
    "\n",
    "    if remove_all_digit:\n",
    "        tokens = [t for t in tokens if not t.isdigit()]\n",
    "\n",
    "    if remove_any_digit:\n",
    "\n",
    "        def has_a_digit(i):\n",
    "            for char in i:\n",
    "                if char.isdigit():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        tokens = [\n",
    "            t for t in tokens if not has_a_digit(i)\n",
    "        ]  # any(map(str.isdigit, list(t)))]\n",
    "\n",
    "    if list_dict_word:\n",
    "        tokens = [t for t in tokens if t in list_dict_word]\n",
    "\n",
    "    if list_extra_stop_word:\n",
    "        tokens = [t for t in tokens if t not in list_extra_stop_word]\n",
    "\n",
    "    if remove_duplicate:\n",
    "        tokens = list(set(tokens))\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nltk_tokenizer(doc)\n",
    "print(res)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize With Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with spacy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm so happy to live here, because this will be the most beautiful place on earth!!!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m(doc)\n\u001b[1;32m      3\u001b[0m tokens\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "doc = \"I'm so happy to live here, because this will be the most beautiful place on earth!!!\"\n",
    "tokens = nlp(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "    print(f\"{t} => {t.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name Entity recognition : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "    print(f\"{t} => {t.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it with Paris : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in nlp(\"i live in Paris\"):\n",
    "    print(f\"{t} => {t.ent_type_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in nlp(\"i am in love with Paris Hilton\"):\n",
    "    print(f\"{t} => {t.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of tokens : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "tokens = [t for t in tokens if not t.is_stop]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Punctuation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[2].is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "tokens = [t for t in tokens if not t.is_punct]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is Digit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "tokens = [t for t in tokens if not t.text.isdigit()]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech  : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "\n",
    "pos_list = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\n",
    "\n",
    "tokens = [t for t in tokens if t.pos_ in pos_list]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmentization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "\n",
    "tokens = [t.lemma_ for t in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(\n",
    "    doc,\n",
    "    len_min_word=3,\n",
    "    force_lower=True,\n",
    "    remove_stop_words=True,\n",
    "    remove_punct=True,\n",
    "    remove_digit_token=True,\n",
    "    remove_all_digit=True,\n",
    "    pos_list=[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"],\n",
    "    lemmentize=True,\n",
    "    list_dict_word=None,\n",
    "    list_extra_stop_word=None,\n",
    "):\n",
    "    doc = doc.lower() if force_lower else doc\n",
    "\n",
    "    doc = doc.strip()\n",
    "\n",
    "    tokens = nlp(doc)\n",
    "\n",
    "    if remove_stop_words:\n",
    "        tokens = [t for t in tokens if not t.is_stop]\n",
    "\n",
    "    if remove_punct:\n",
    "        tokens = [t for t in tokens if not t.is_punct]\n",
    "\n",
    "    tokens = [t for t in tokens if len(t) >= len_min_word]\n",
    "\n",
    "    if remove_digit_token:\n",
    "        tokens = [t for t in tokens if not t.text.isdigit()]\n",
    "\n",
    "    if remove_all_digit:\n",
    "        tokens = [t for t in tokens if not any(map(str.isdigit, list(t.text)))]\n",
    "\n",
    "    if pos_list:\n",
    "        tokens = [t for t in tokens if t.pos_ in pos_list]\n",
    "\n",
    "    if lemmentize:\n",
    "        tokens = [t.lemma_ for t in tokens]\n",
    "\n",
    "    if list_dict_word:\n",
    "        tokens = [t for t in tokens if t.text in list_dict_word]\n",
    "\n",
    "    if list_extra_stop_word:\n",
    "        tokens = [t for t in tokens if t.text not in list_extra_stop_word]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df.description.iloc[0]\n",
    "res = spacy_tokenizer(doc)\n",
    "print(res)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer and TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an artificial corpus : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"my cat is red\",\n",
    "    \"my cat is blue\",\n",
    "    \"my cat is yellow, i know that is wierd but he is yellow, yellow, yellow\",\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a pd.Series : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.Series(corpus, name=\"text\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init a Count Vectorizer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "X = cv.fit(corpus)\n",
    "X = cv.transform(corpus).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull dataframe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=cv.get_feature_names_out())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with TFIDF : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.fit_transform(corpus).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=tf.get_feature_names_out())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:18.319086Z",
     "start_time": "2023-02-23T06:59:18.226388Z"
    }
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "X = tf.fit_transform(df.description).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:18.981572Z",
     "start_time": "2023-02-23T06:59:18.933709Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=tf.get_feature_names_out())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:19.379883Z",
     "start_time": "2023-02-23T06:59:19.373821Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df.cat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a much more advanced cross validation tool : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:19.756247Z",
     "start_time": "2023-02-23T06:59:19.748349Z"
    }
   },
   "outputs": [],
   "source": [
    "def cv():\n",
    "    return StratifiedShuffleSplit(n_splits=10, test_size=0.25)\n",
    "\n",
    "\n",
    "cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grid Search : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:59.289390Z",
     "start_time": "2023-02-23T06:59:59.140674Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    LogisticRegression(), {}, cv=10, n_jobs=-1, verbose=1, return_train_score=True\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:00:02.474316Z",
     "start_time": "2023-02-23T07:00:02.457408Z"
    }
   },
   "outputs": [],
   "source": [
    "def resultize(grid):\n",
    "\n",
    "    res = pd.DataFrame(grid.cv_results_)\n",
    "    cols = [i for i in res.columns if \"split\" not in i]\n",
    "    res = res.loc[:, cols]\n",
    "    res = res.round(2).sort_values(\"mean_test_score\", ascending=False).head(10)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        # (\"scaler\", StandardScaler()),\n",
    "        # (\"reductor\", TruncatedSVD(n_components=100)),\n",
    "        (\"estimator\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"estimator\": [\n",
    "        # RandomForestClassifier(),\n",
    "        # # KNeighborsClassifier(),\n",
    "        # LGBMClassifier(),\n",
    "        # XGBClassifier(),\n",
    "        # XGBRFClassifier(),\n",
    "        LogisticRegression(),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid, cv=10, n_jobs=-1, return_train_score=True, verbose=1\n",
    ")\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = \"passthrough\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        (\"imputer\", pst),\n",
    "        (\"scaler\", pst),\n",
    "        (\"reductor\", pst),\n",
    "        (\"estimator\", DummyClassifier()),\n",
    "    ]\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor\": [TfidfVectorizer(), CountVectorizer()],\n",
    "    \"scaler\": [\n",
    "        StandardScaler(),\n",
    "        QuantileTransformer(n_quantiles=100),\n",
    "        # MinMaxScaler(),\n",
    "        Normalizer(),\n",
    "        # RobustScaler(),\n",
    "    ],\n",
    "    \"imputer\": [\n",
    "        pst,\n",
    "    ],\n",
    "    \"reductor\": [\n",
    "        pst,\n",
    "    ],\n",
    "    \"estimator\": [\n",
    "        # KNeighborsClassifier(),\n",
    "        # LGBMClassifier(),\n",
    "        # XGBClassifier(),\n",
    "        LogisticRegression(),\n",
    "        RandomForestClassifier(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid, cv=10, n_jobs=-1, verbose=1, return_train_score=True\n",
    ")\n",
    "\n",
    "display(grid)\n",
    "\n",
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Reductor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor\": [\n",
    "        TfidfVectorizer(),\n",
    "    ],  # CountVectorizer()\n",
    "    \"scaler\": [\n",
    "        pst,\n",
    "        # StandardScaler(),\n",
    "        QuantileTransformer(n_quantiles=100),\n",
    "        Normalizer(),\n",
    "    ],  # MinMaxScaler() RobustScaler()\n",
    "    \"imputer\": [pst],\n",
    "    \"reductor\": [\n",
    "        TruncatedSVD(),\n",
    "    ],\n",
    "    \"estimator\": [\n",
    "        # KNeighborsClassifier(),\n",
    "        # XGBRFClassifier(),\n",
    "        # LGBMClassifier(),\n",
    "        # XGBClassifier(),\n",
    "        LogisticRegression(),\n",
    "        RandomForestClassifier(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid, cv=cv(), n_jobs=-1, verbose=1, return_train_score=True\n",
    ")\n",
    "\n",
    "display(grid)\n",
    "\n",
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid.best_estimator_)\n",
    "\n",
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor\": [\n",
    "        TfidfVectorizer(),\n",
    "    ],  # CountVectorizer()\n",
    "    \"scaler\": [\n",
    "        pst,\n",
    "        # StandardScaler(),\n",
    "        QuantileTransformer(n_quantiles=100),\n",
    "        Normalizer(),\n",
    "    ],  # MinMaxScaler() RobustScaler()\n",
    "    \"imputer\": [pst],\n",
    "    \"reductor\": [\n",
    "        TruncatedSVD(),\n",
    "    ],\n",
    "    \"reductor__n_components\": np.linspace(2, 100, 5).astype(int),\n",
    "    \"estimator\": [\n",
    "        # KNeighborsClassifier(),\n",
    "        # # XGBRFClassifier(),\n",
    "        # LGBMClassifier(),\n",
    "        # XGBClassifier(),\n",
    "        LogisticRegression(),\n",
    "        RandomForestClassifier(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid, cv=cv(), n_jobs=-1, verbose=1, return_train_score=True\n",
    ")\n",
    "\n",
    "display(grid)\n",
    "\n",
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid.best_estimator_)\n",
    "\n",
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Advanced Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NltkTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        len_min_word=3,\n",
    "        force_lower=True,\n",
    "        remove_stop_words=True,\n",
    "        remove_punct=True,\n",
    "        remove_digit_token=True,\n",
    "        remove_all_digit=True,\n",
    "        list_dict_word=None,\n",
    "        list_extra_stop_word=None,\n",
    "    ):\n",
    "        self.len_min_word = len_min_word\n",
    "        self.force_lower = force_lower\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_punct = remove_punct\n",
    "        self.remove_digit_token = remove_digit_token\n",
    "        self.remove_all_digit = remove_all_digit\n",
    "        self.list_dict_word = list_dict_word\n",
    "        self.list_extra_stop_word = list_extra_stop_word\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        f = lambda i: nltk_tokenizer(\n",
    "            i,\n",
    "            len_min_word=self.len_min_word,\n",
    "            force_lower=self.force_lower,\n",
    "            remove_stop_words=self.remove_stop_words,\n",
    "            remove_punct=self.remove_punct,\n",
    "            # remove_digit_token=self.remove_digit_token,\n",
    "            remove_all_digit=self.remove_all_digit,\n",
    "            list_dict_word=self.list_dict_word,\n",
    "            list_extra_stop_word=self.list_extra_stop_word,\n",
    "        )\n",
    "\n",
    "        return X.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = NltkTokenizer().fit_transform(df.description)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tokenizer\", NltkTokenizer()),\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        (\"estimator\", LogisticRegression()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"tokenizer__force_lower\": [True, False],\n",
    "    \"tokenizer__len_min_word\": [1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid, cv=10, n_jobs=-1, return_train_score=True, verbose=1\n",
    ")\n",
    "\n",
    "display(grid)\n",
    "\n",
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid.best_estimator_)\n",
    "\n",
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        len_min_word=3,\n",
    "        force_lower=True,\n",
    "        remove_stop_words=True,\n",
    "        remove_punct=True,\n",
    "        remove_digit_token=True,\n",
    "        remove_all_digit=True,\n",
    "        list_dict_word=None,\n",
    "        list_extra_stop_word=None,\n",
    "    ):\n",
    "        self.len_min_word = len_min_word\n",
    "        self.force_lower = force_lower\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_punct = remove_punct\n",
    "        self.remove_digit_token = remove_digit_token\n",
    "        self.remove_all_digit = remove_all_digit\n",
    "        self.list_dict_word = list_dict_word\n",
    "        self.list_extra_stop_word = list_extra_stop_word\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        f = lambda i: spacy_tokenizer(\n",
    "            i,\n",
    "            len_min_word=self.len_min_word,\n",
    "            force_lower=self.force_lower,\n",
    "            remove_stop_words=self.remove_stop_words,\n",
    "            remove_punct=self.remove_punct,\n",
    "            remove_digit_token=self.remove_digit_token,\n",
    "            remove_all_digit=self.remove_all_digit,\n",
    "            list_dict_word=self.list_dict_word,\n",
    "            list_extra_stop_word=self.list_extra_stop_word,\n",
    "        )\n",
    "\n",
    "        return X.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = SpacyTokenizer().fit_transform(df.description)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tokenizer\", SpacyTokenizer()),\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        (\"estimator\", LogisticRegression()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipeline, {}, cv=10, n_jobs=-1, return_train_score=True, verbose=1)\n",
    "\n",
    "display(grid)\n",
    "\n",
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid.best_estimator_)\n",
    "\n",
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd158c7f1a670d1d188328ca41323458089a3c8a74c42040854dc6ec477fbf56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
