{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 012-03 - NLP Text Processing - Solution Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Written by Alexandre Gazagnes\n",
    "* Last update: 2024-02-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna implement our 1st NLP tool ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data  : \n",
    "\n",
    "**You can find the dataset [here](https://gist.githubusercontent.com/AlexandreGazagnes/cabe445634a092d308d17a883a305a75/raw/9f785f0f02739ac6352e1d583323771d55270221/nlp.csv).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "PBabb1xmqSlM"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dAhj129JqdaC"
   },
   "source": [
    "### System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands will display the system information:\n",
    "\n",
    "Uncomment theses lines if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "yE-usmhkycvz"
   },
   "source": [
    "Install various Librairies : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:57.038920Z",
     "start_time": "2023-02-23T06:58:57.032717Z"
    },
    "hidden": true,
    "id": "sfsFooZVqajD"
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt >> pip.log\n",
    "# !pip freeze >> pip.freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "xVpywqWAqalg"
   },
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:59.595289Z",
     "start_time": "2023-02-23T06:58:57.672688Z"
    },
    "hidden": true,
    "id": "Rt4va8LsqnGR"
   },
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "import pickle\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.impute import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.feature_extraction import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# from lightgbm import *\n",
    "# from xgboost import *\n",
    "\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# import wordcloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "xkZ4K9CRqaoM"
   },
   "source": [
    "### Graphs and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:59.600629Z",
     "start_time": "2023-02-23T06:58:59.597369Z"
    },
    "hidden": true,
    "id": "34QZa76rqxLu"
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:58:59.604999Z",
     "start_time": "2023-02-23T06:58:59.602475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action=\"once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed we can use a TEST_MODE to run the notebook to have a very fast execution : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 10  # number of folds for the  cross val\n",
    "N_JOBS = 7  # number of cpu to use for computations\n",
    "FRAC = 1.0  # we keep 100% of the dataframe\n",
    "DISPLAY = True  # display complex viz\n",
    "TEST_SIZE = 0.25  # Train vs Test %\n",
    "\n",
    "if TEST_MODE:\n",
    "    CV = 3\n",
    "    N_JOBS = -1\n",
    "    FRAC = 0.3\n",
    "    DISPLAY = False\n",
    "    TEST_SIZE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Thrid Parties Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some Third parties : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:00.115507Z",
     "start_time": "2023-02-23T06:59:00.044665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some string assets : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:00.491350Z",
     "start_time": "2023-02-23T06:59:00.420416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "# stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(set(string.punctuation))\n",
    "punctuation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = words.words()\n",
    "word_dict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download spacy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:01.023888Z",
     "start_time": "2023-02-23T06:59:01.016385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load spacy model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:01.130801Z",
     "start_time": "2023-02-23T06:59:01.128456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "FUVeEfm9qx8f"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url of the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:01.819218Z",
     "start_time": "2023-02-23T06:59:01.811890Z"
    },
    "hidden": true,
    "id": "o_SlDzD1r4KP"
   },
   "outputs": [],
   "source": [
    "url = \"https://gist.githubusercontent.com/AlexandreGazagnes/cabe445634a092d308d17a883a305a75/raw/d2014e8a34bba3c1be3ec8936bb338fb42888f24/nlp.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:02.206962Z",
     "start_time": "2023-02-23T06:59:02.172398Z"
    },
    "hidden": true,
    "id": "4LfAEaWRrzTz"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed let's take just a specific % of the dataframe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    df = df.sample(frac=FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cat_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a copy of the df : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T14:50:27.970009Z",
     "start_time": "2023-02-10T14:50:27.432446Z"
    },
    "heading_collapsed": true,
    "id": "kzoLQvf44HRX"
   },
   "source": [
    "## First tour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 10 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:04.207912Z",
     "start_time": "2023-02-23T06:59:04.190392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value counts : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific data types : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude=np.number).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Nan & Dupliacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any missing values : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = df.isna().mean(axis=0)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = df.isna().mean(axis=1)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any duplicated : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numerical stats : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other stats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.describe(include=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select numeric : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(np.number).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select non numeric : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(object).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Text Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[print(i + \"\\n\\n\") for i in df.description.head().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[print(i + \"\\n\\n\") for i in df.description.tail().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[print(i + \"\\n\\n\") for i in df.description.sample(10).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display by cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "\n",
    "key = df.cat_1.unique()[i]\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------------------\")\n",
    "print(f\"-------------- {key} --------------- \")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "tmp = df.loc[df.cat_1 == key, :]\n",
    "[print(i[:lim] + \"\\n\\n\") for i in tmp.description.head(5).values]\n",
    "[print(i[:lim] + \"\\n\\n\") for i in tmp.description.sample(5).values]\n",
    "[print(i[:lim] + \"\\n\\n\") for i in tmp.description.tail(5).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_categ(i):\n",
    "\n",
    "    key = df.cat_1.unique()[i]\n",
    "\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(f\"-------------- {key} --------------- \")\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    tmp = df.loc[df.cat_1 == key, :]\n",
    "    [print(i[:lim] + \"\\n\\n\") for i in tmp.description.head(5).values]\n",
    "    [print(i[:lim] + \"\\n\\n\") for i in tmp.description.sample(5).values]\n",
    "    [print(i[:lim] + \"\\n\\n\") for i in tmp.description.tail(5).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Categ 1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_categ(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Categ 2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_categ(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Text To Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create doc from 1st description : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df.description.iloc[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Many Tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Stop words : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our punctuation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English dictionnary (lower) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict\n",
    "word_dict = [i.lower() for i in word_dict]\n",
    "word_dict[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenizer(\n",
    "    doc: str,\n",
    "    len_min_word: int = 3,\n",
    "    force_lower: bool = True,\n",
    "    remove_stop_words=True,\n",
    "    remove_punct=True,\n",
    "    remove_all_digit=True,\n",
    "    remove_any_digit=False,\n",
    "    list_dict_word=None,\n",
    "    list_extra_stop_word=None,\n",
    "    remove_duplicate=False,\n",
    ") -> str:\n",
    "\n",
    "    if force_lower:\n",
    "        doc = doc.lower()  # if force_lower else doc\n",
    "\n",
    "    doc = doc.strip()\n",
    "\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "\n",
    "    if remove_stop_words:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    if remove_punct:\n",
    "        tokens = [t for t in tokens if t not in punctuation]\n",
    "\n",
    "    if len_min_word > 0:\n",
    "        tokens = [t for t in tokens if len(t) >= len_min_word]\n",
    "\n",
    "    if remove_all_digit:\n",
    "        tokens = [t for t in tokens if not t.isdigit()]\n",
    "\n",
    "    if remove_any_digit:\n",
    "\n",
    "        def has_a_digit(i):\n",
    "            for char in i:\n",
    "                if char.isdigit():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        tokens = [\n",
    "            t for t in tokens if not has_a_digit(i)\n",
    "        ]  # any(map(str.isdigit, list(t)))]\n",
    "\n",
    "    if list_dict_word:\n",
    "        tokens = [t for t in tokens if t in list_dict_word]\n",
    "\n",
    "    if list_extra_stop_word:\n",
    "        tokens = [t for t in tokens if t not in list_extra_stop_word]\n",
    "\n",
    "    if remove_duplicate:\n",
    "        tokens = list(set(tokens))\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nltk_tokenizer(doc)\n",
    "print(res)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize With Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with spacy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here, because this will be the most beautiful place on earth!!!\"\n",
    "tokens = nlp(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "    print(f\"{t} => {t.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name Entity recognition : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "    print(f\"{t} => {t.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it with Paris : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in nlp(\"i live in Paris\"):\n",
    "    print(f\"{t} => {t.ent_type_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in nlp(\"i am in love with Paris Hilton\"):\n",
    "    print(f\"{t} => {t.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of tokens : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "tokens = [t for t in tokens if not t.is_stop]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Punctuation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[2].is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "tokens = [t for t in tokens if not t.is_punct]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is Digit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "tokens = [t for t in tokens if not t.text.isdigit()]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech  : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "\n",
    "pos_list = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\n",
    "\n",
    "tokens = [t for t in tokens if t.pos_ in pos_list]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmentization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I'm so happy to live here because this will be the most beautiful place on earth\"\n",
    "tokens = nlp(doc)\n",
    "\n",
    "tokens = [t.lemma_ for t in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(\n",
    "    doc,\n",
    "    len_min_word=3,\n",
    "    force_lower=True,\n",
    "    remove_stop_words=True,\n",
    "    remove_punct=True,\n",
    "    remove_digit_token=True,\n",
    "    remove_all_digit=True,\n",
    "    pos_list=[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"],\n",
    "    lemmentize=True,\n",
    "    list_dict_word=None,\n",
    "    list_extra_stop_word=None,\n",
    "):\n",
    "    doc = doc.lower() if force_lower else doc\n",
    "\n",
    "    doc = doc.strip()\n",
    "\n",
    "    tokens = nlp(doc)\n",
    "\n",
    "    if remove_stop_words:\n",
    "        tokens = [t for t in tokens if not t.is_stop]\n",
    "\n",
    "    if remove_punct:\n",
    "        tokens = [t for t in tokens if not t.is_punct]\n",
    "\n",
    "    tokens = [t for t in tokens if len(t) >= len_min_word]\n",
    "\n",
    "    if remove_digit_token:\n",
    "        tokens = [t for t in tokens if not t.text.isdigit()]\n",
    "\n",
    "    if remove_all_digit:\n",
    "        tokens = [t for t in tokens if not any(map(str.isdigit, list(t.text)))]\n",
    "\n",
    "    if pos_list:\n",
    "        tokens = [t for t in tokens if t.pos_ in pos_list]\n",
    "\n",
    "    if lemmentize:\n",
    "        tokens = [t.lemma_ for t in tokens]\n",
    "\n",
    "    if list_dict_word:\n",
    "        tokens = [t for t in tokens if t.text in list_dict_word]\n",
    "\n",
    "    if list_extra_stop_word:\n",
    "        tokens = [t for t in tokens if t.text not in list_extra_stop_word]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df.description.iloc[0]\n",
    "res = spacy_tokenizer(doc)\n",
    "print(res)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer and TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an artificial corpus : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"my cat is red\",\n",
    "    \"my cat is blue\",\n",
    "    \"my cat is yellow, i know that is wierd but he is yellow, yellow, yellow\",\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a pd.Series : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.Series(corpus, name=\"text\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init a Count Vectorizer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "X = cv.fit(corpus)\n",
    "X = cv.transform(corpus).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull dataframe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=cv.get_feature_names_out())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with TFIDF : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.fit_transform(corpus).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=tf.get_feature_names_out())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:18.319086Z",
     "start_time": "2023-02-23T06:59:18.226388Z"
    }
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "X = tf.fit_transform(df.description).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:18.981572Z",
     "start_time": "2023-02-23T06:59:18.933709Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=tf.get_feature_names_out())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:19.379883Z",
     "start_time": "2023-02-23T06:59:19.373821Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df.cat_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a much more advanced cross validation tool : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:19.756247Z",
     "start_time": "2023-02-23T06:59:19.748349Z"
    }
   },
   "outputs": [],
   "source": [
    "def cv():\n",
    "    return StratifiedShuffleSplit(n_splits=CV, test_size=TEST_SIZE)\n",
    "\n",
    "\n",
    "cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grid Search : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T06:59:59.289390Z",
     "start_time": "2023-02-23T06:59:59.140674Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    LogisticRegression(),\n",
    "    {},\n",
    "    cv=CV,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:00:02.474316Z",
     "start_time": "2023-02-23T07:00:02.457408Z"
    }
   },
   "outputs": [],
   "source": [
    "def resultize(grid):\n",
    "\n",
    "    res = grid.cv_results_\n",
    "    res = pd.DataFrame(res)\n",
    "\n",
    "    cols = [i for i in res.columns if \"split\" not in i]\n",
    "    res = res.loc[:, cols]\n",
    "\n",
    "    res = res.drop(columns=[\"mean_score_time\", \"std_score_time\"])\n",
    "\n",
    "    return res.round(2).sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        # (\"scaler\", StandardScaler()),\n",
    "        # (\"reductor\", TruncatedSVD(n_components=100)),\n",
    "        (\"estimator\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"estimator\": [\n",
    "        RandomForestClassifier(),\n",
    "        # # KNeighborsClassifier(),\n",
    "        # LGBMClassifier(),\n",
    "        # XGBClassifier(),\n",
    "        # XGBRFClassifier(),\n",
    "        LogisticRegression(),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = \"passthrough\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        (\"imputer\", pst),\n",
    "        (\"scaler\", pst),\n",
    "        (\"reductor\", pst),\n",
    "        (\"estimator\", DummyClassifier()),\n",
    "    ]\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"scaler\": [\n",
    "        StandardScaler(),\n",
    "        QuantileTransformer(n_quantiles=100),\n",
    "        # MinMaxScaler(),\n",
    "        Normalizer(),\n",
    "        # RobustScaler(),\n",
    "    ],\n",
    "    \"imputer\": [\n",
    "        pst,\n",
    "    ],\n",
    "    \"reductor\": [\n",
    "        pst,\n",
    "    ],\n",
    "    \"estimator\": [LogisticRegression(), RandomForestClassifier()],\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(df.description.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Reductor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor\": [TfidfVectorizer(), CountVectorizer()],  #\n",
    "    \"scaler\": [\n",
    "        pst,\n",
    "        StandardScaler(),\n",
    "        QuantileTransformer(n_quantiles=100),\n",
    "        Normalizer(),\n",
    "    ],  # MinMaxScaler() RobustScaler()\n",
    "    \"imputer\": [pst],\n",
    "    \"reductor\": [TruncatedSVD(n_components=100)],\n",
    "    \"estimator\": [\n",
    "        # KNeighborsClassifier(),\n",
    "        # XGBRFClassifier(),\n",
    "        # LGBMClassifier(),\n",
    "        # XGBClassifier(),\n",
    "        LogisticRegression(),\n",
    "        RandomForestClassifier(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor\": [TfidfVectorizer(), CountVectorizer()],  #\n",
    "    \"scaler\": [\n",
    "        pst,\n",
    "        StandardScaler(),\n",
    "        QuantileTransformer(n_quantiles=100),\n",
    "        Normalizer(),\n",
    "    ],  # MinMaxScaler() RobustScaler()\n",
    "    \"imputer\": [pst],\n",
    "    \"reductor\": [TruncatedSVD()],\n",
    "    \"reductor__n_components\": np.linspace(10, 1_000, 10).astype(int),\n",
    "    \"estimator\": [\n",
    "        # KNeighborsClassifier(),\n",
    "        # # XGBRFClassifier(),\n",
    "        # LGBMClassifier(),\n",
    "        # XGBClassifier(),\n",
    "        LogisticRegression(),\n",
    "        RandomForestClassifier(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Advanced Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NltkTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        len_min_word=3,\n",
    "        force_lower=True,\n",
    "        remove_stop_words=True,\n",
    "        remove_punct=True,\n",
    "        remove_digit_token=True,\n",
    "        remove_all_digit=True,\n",
    "        list_dict_word=None,\n",
    "        list_extra_stop_word=None,\n",
    "    ):\n",
    "        self.len_min_word = len_min_word\n",
    "        self.force_lower = force_lower\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_punct = remove_punct\n",
    "        self.remove_digit_token = remove_digit_token\n",
    "        self.remove_all_digit = remove_all_digit\n",
    "        self.list_dict_word = list_dict_word\n",
    "        self.list_extra_stop_word = list_extra_stop_word\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        f = lambda i: nltk_tokenizer(\n",
    "            i,\n",
    "            len_min_word=self.len_min_word,\n",
    "            force_lower=self.force_lower,\n",
    "            remove_stop_words=self.remove_stop_words,\n",
    "            remove_punct=self.remove_punct,\n",
    "            # remove_digit_token=self.remove_digit_token,\n",
    "            remove_all_digit=self.remove_all_digit,\n",
    "            list_dict_word=self.list_dict_word,\n",
    "            list_extra_stop_word=self.list_extra_stop_word,\n",
    "        )\n",
    "\n",
    "        return X.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = NltkTokenizer().fit_transform(df.description)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tokenizer\", NltkTokenizer()),\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        (\"estimator\", LogisticRegression()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"tokenizer__force_lower\": [True, False],\n",
    "    \"tokenizer__len_min_word\": [1, 2, 3, 4, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        len_min_word=3,\n",
    "        force_lower=True,\n",
    "        remove_stop_words=True,\n",
    "        remove_punct=True,\n",
    "        remove_digit_token=True,\n",
    "        remove_all_digit=True,\n",
    "        list_dict_word=None,\n",
    "        list_extra_stop_word=None,\n",
    "    ):\n",
    "        self.len_min_word = len_min_word\n",
    "        self.force_lower = force_lower\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_punct = remove_punct\n",
    "        self.remove_digit_token = remove_digit_token\n",
    "        self.remove_all_digit = remove_all_digit\n",
    "        self.list_dict_word = list_dict_word\n",
    "        self.list_extra_stop_word = list_extra_stop_word\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        f = lambda i: spacy_tokenizer(\n",
    "            i,\n",
    "            len_min_word=self.len_min_word,\n",
    "            force_lower=self.force_lower,\n",
    "            remove_stop_words=self.remove_stop_words,\n",
    "            remove_punct=self.remove_punct,\n",
    "            remove_digit_token=self.remove_digit_token,\n",
    "            remove_all_digit=self.remove_all_digit,\n",
    "            list_dict_word=self.list_dict_word,\n",
    "            list_extra_stop_word=self.list_extra_stop_word,\n",
    "        )\n",
    "\n",
    "        return X.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = SpacyTokenizer().fit_transform(df.description)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tokenizer\", SpacyTokenizer()),\n",
    "        (\"preprocessor\", TfidfVectorizer()),\n",
    "        (\"estimator\", LogisticRegression()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=CV,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.fit(df.description, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultize(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd158c7f1a670d1d188328ca41323458089a3c8a74c42040854dc6ec477fbf56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
